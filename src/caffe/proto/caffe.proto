// Copyright 2014 BVLC and contributors.

package caffe;

message BlobProto {
  optional int32 num = 1 [default = 0];
  optional int32 channels = 2 [default = 0];
  optional int32 height = 3 [default = 0];
  optional int32 width = 4 [default = 0];
  repeated float data = 5 [packed = true];
  repeated float diff = 6 [packed = true];
}

// The BlobProtoVector is simply a way to pass multiple blobproto instances
// around.
message BlobProtoVector {
  repeated BlobProto blobs = 1;
}

message Datum {
  optional int32 channels = 1;
  optional int32 height = 2;
  optional int32 width = 3;
  // the actual image data, in bytes
  optional bytes data = 4;
  optional int32 label = 5;
  // Optionally, the datum could also hold float data.
  repeated float float_data = 6;
}

message FillerParameter {
  // The filler type.
  optional string type = 1 [default = 'constant'];
  optional float value = 2 [default = 0]; // the value in constant filler
  optional float min = 3 [default = 0]; // the min value in uniform filler
  optional float max = 4 [default = 1]; // the max value in uniform filler
  optional float mean = 5 [default = 0]; // the mean value in Gaussian filler
  optional float std = 6 [default = 1]; // the std value in Gaussian filler
  // The expected number of non-zero input weights for a given output in
  // Gaussian filler -- the default -1 means don't perform sparsification.
  optional int32 sparse = 7 [default = -1];
  optional float xavier_coeff = 8 [default = 2];
}

message NetParameter {
  optional string name = 1; // consider giving the network a name
  repeated LayerParameter layers = 2; // a bunch of layers.
  // The input blobs to the network.
  repeated string input = 3;
  // The dim of the input blobs. For each input blob there should be four
  // values specifying the num, channels, height and width of the input blob.
  // Thus, there should be a total of (4 * #input) numbers.
  repeated int32 input_dim = 4;
  // Whether the network will force every layer to carry out backward operation.
  // If set False, then whether to carry out backward is determined
  // automatically according to the net structure and learning rates.
  optional bool force_backward = 5 [default = false];
}

message SolverParameter {
  // {train,test}_net specify a path to a file containing the {train,test} net
  // parameters; {train,test}_net_param specify the net parameters directly
  // inside the SolverParameter.
  //
  // Only either train_net or train_net_param (not both) should be specified.
  // You may specify 0 or more test_net and/or test_net_param.  All
  // nets specified using test_net_param will be tested first, followed by all
  // nets specified using test_net (each processed in the order specified in
  // the prototxt).
  optional string train_net = 1; // The proto filename for the train net.
  repeated string test_net = 2; // The proto filenames for the test nets.
  optional NetParameter train_net_param = 21; // Full params for the train net.
  repeated NetParameter test_net_param = 22; // Full params for the test nets.
  // The number of iterations for each testing phase.
  repeated int32 test_iter = 3;
  // The number of iterations between two testing phases.
  optional int32 test_interval = 4 [default = 0];
  optional bool test_compute_loss = 19 [default = false];
  optional float base_lr = 5; // The base learning rate
  // the number of iterations between displaying info. If display = 0, no info
  // will be displayed.
  optional int32 display = 6;
  optional int32 max_iter = 7; // the maximum number of iterations
  optional string lr_policy = 8; // The learning rate decay policy.
  optional float gamma = 9; // The parameter to compute the learning rate.
  optional float power = 10; // The parameter to compute the learning rate.
  optional float momentum = 11; // The momentum value.
  optional float weight_decay = 12; // The weight decay.
  optional bool weight_constraint = 28 [default = false];  // whether to apply a constraint on the l2 norm
  optional int32 stepsize = 13; // the stepsize for learning rate policy "step"
  optional int32 snapshot = 14 [default = 0]; // The snapshot interval
  optional string snapshot_prefix = 15; // The prefix for the snapshot.
  // whether to snapshot diff in the results or not. Snapshotting diff will help
  // debugging but the final protocol buffer size will be much larger.
  optional bool snapshot_diff = 16 [default = false];
  // the mode solver will use: 0 for CPU and 1 for GPU. Use GPU in default.
  enum SolverMode {
    CPU = 0;
    GPU = 1;
  }
  optional SolverMode solver_mode = 17 [default = GPU];
  // the device_id will that be used in GPU mode. Use device_id = 0 in default.
  optional int32 device_id = 18 [default = 0];
  // If non-negative, the seed with which the Solver will initialize the Caffe
  // random number generator -- useful for reproducible results. Otherwise,
  // (and by default) initialize using a seed derived from the system clock.
  optional int64 random_seed = 20 [default = -1];
  enum TerminationCriterion {
    MAX_ITER = 0; // terminates after max_iter
    TEST_ACCURACY = 1; // terminates when the test set accuracy doesn't improve for test_accuracy_stop_countdown iterations
    EXTERNAL = 2;
    EXTERNAL_IN_BACKGROUND = 3;
    DIVERGENCE_DETECTION = 4;
  }
  repeated TerminationCriterion termination_criterion = 23; // The condition that has to be met for the training to stop
  optional int32 test_accuracy_stop_countdown = 24 [default = 10]; // the number of iterations of non-increasing test accuracy before stopping
  optional int32 snapshot_on_exit = 25 [default = 0];
  optional string external_term_criterion_cmd = 26;
  optional int32 external_term_criterion_num_iter = 27;
  optional string load_weights_from = 29;
  
  repeated float step_lr = 30;
  repeated uint32 step_iter = 31;
}

// A message that stores the solver snapshots
message SolverState {
  optional int32 iter = 1; // The current iteration
  optional string learned_net = 2; // The file that stores the learned net.
  repeated BlobProto history = 3; // The history for sgd solvers
}

// NOTE
// Update the next available ID when you add a new LayerParameter field.
//
// LayerParameter next available ID: 28 (last added: dummy_data_param)
message LayerParameter {
  repeated string bottom = 2; // the name of the bottom blobs
  repeated string top = 3; // the name of the top blobs
  optional string name = 4; // the layer name

  // NOTE
  // Add new LayerTypes to the enum below in lexicographical order (other than
  // starting with NONE), starting with the next available ID in the comment
  // line above the enum. Update the next available ID when you add a new
  // LayerType.
  //
  // LayerType next available ID: 37 (last added: DATA_AUGMENTATION)
  enum LayerType {
    // "NONE" layer type is 0th enum element so that we don't cause confusion
    // by defaulting to an existent LayerType (instead, should usually error if
    // the type is unspecified).
    NONE = 0;
    ACCURACY = 1;
    ARGMAX = 30;
    BNLL = 2;
    CONCAT = 3;
    CONVOLUTION = 4;
    DATA = 5;
    DATA_AUGMENTATION = 36;
    DATA_LOAD_AND_AUGMENT = 37;
    DROPOUT = 6;
    DUMMY_DATA = 32;
    EUCLIDEAN_LOSS = 7;
    ELTWISE = 25;
    FLATTEN = 8;
    HDF5_DATA = 9;
    HDF5_OUTPUT = 10;
    HINGE_LOSS = 28;
    IM2COL = 11;
    IMAGE_DATA = 12;
    INFOGAIN_LOSS = 13;
    INNER_PRODUCT = 14;
    LRN = 15;
    MEMORY_DATA = 29;
    MULTINOMIAL_LOGISTIC_LOSS = 16;
    POOLING = 17;
    POWER = 26;
    RELU = 18;
    SIGMOID = 19;
    SIGMOID_CROSS_ENTROPY_LOSS = 27;
    SOFTMAX = 20;
    SOFTMAX_LOSS = 21;
    SPLIT = 22;
    TANH = 23;
    WINDOW_DATA = 24;
    THRESHOLD = 31;
    UNPOOLING = 33;
    DECONVOLUTION = 34;
    SUBSPACEPOOLING = 35;
    INNER_PRODUCT_ORTH = 38;
    LABEL_TO_ONEHOT = 39;
    SOFTMAX_MULTILABEL_LOSS = 40;
    CONVOLUTION_ORTH = 41;
    HDF5_DATA_COUPLES = 42;
    SOFTMAX_KL_LOSS = 43;
    SOFTMAX_ENTROPY_LOSS = 44;
    ENTROPY_LOSS = 45;
    ACCUM = 46;
    RESHAPE = 47;
    SOFTMAX_CONDITION_LOSS = 48;
    IM2PERPIXEL = 49;
    INVERT_GRADIENT = 50;
    DECONVOLUTION_ORTH = 51;
  }
  optional LayerType type = 5; // the layer type from the enum above

  // The blobs containing the numeric parameters of the layer
  repeated BlobProto blobs = 6;
  // The ratio that is multiplied on the global learning rate. If you want to
  // set the learning ratio for one blob, you need to set it for all blobs.
  repeated float blobs_lr = 7;
  // The weight decay that is multiplied on the global weight decay.
  repeated float weight_decay = 8;
  // the max norm constraint that is used for this layer
  optional float max_weight_norm = 31 [default = 1.];

  optional ArgMaxParameter argmax_param = 23;
  optional AugmentationParameter augmentation_param = 30;
  optional ConcatParameter concat_param = 9;
  optional ConvolutionParameter convolution_param = 10;
  optional DeConvolutionParameter deconvolution_param = 27;
  optional DataParameter data_param = 11;
  optional DropoutParameter dropout_param = 12;
  optional DummyDataParameter dummy_data_param = 26;
  optional EltwiseParameter eltwise_param = 24;
  optional HDF5DataParameter hdf5_data_param = 13;
  optional HDF5OutputParameter hdf5_output_param = 14;
  optional ImageDataParameter image_data_param = 15;
  optional InfogainLossParameter infogain_loss_param = 16;
  optional InnerProductParameter inner_product_param = 17;
  optional LossParameter loss_param = 32;
  optional LRNParameter lrn_param = 18;
  optional MemoryDataParameter memory_data_param = 22;
  optional PoolingParameter pooling_param = 19;
  optional PowerParameter power_param = 21;
  optional WindowDataParameter window_data_param = 20;
  optional ThresholdParameter threshold_param = 25;
  optional HingeLossParameter hinge_loss_param = 29;
  optional OrthParameter orth_param = 33;
  optional CouplesParameter couples_param = 34;
  optional AccumParameter accum_param = 35;
  optional ReshapeParameter reshape_param = 36;
  optional ConditionParameter condition_param = 37;
  optional CoeffScheduleParameter coeff_schedule_param = 38;
  optional ReLUParameter relu_param = 39;
  
  repeated string share_weights_with = 40;
  repeated string initialize_weights_with = 41;
  
  optional bool force_output = 42 [default = false];
  
  

  // DEPRECATED: The layer parameters specified as a V0LayerParameter.
  // This should never be used by any code except to upgrade to the new
  // LayerParameter specification.
  optional V0LayerParameter layer = 1;
}

message AccumParameter {
  optional float discount_coeff = 1 [default = 1.];
  optional float init_alpha = 2 [default = 0.];
}

// Message that stores parameters used by ArgMaxLayer
message ArgMaxParameter {
  // If true produce pairs (argmax, maxval)
  optional bool out_max_val = 1 [default = false];
}

// Message describing distribution of augmentation parameters
message AugmentationParameter {
  optional uint32 crop_size = 1 [default = 0 ];
  optional string write_augmented = 2 [default = ""];
  optional float max_multiplier = 3 [default = 100.];
  optional bool augment_during_test = 4 [default = false];
  optional uint32 recompute_mean = 5 [default = 0]; // number of iterations to recompute mean (0 - do not recompute)
  optional string write_mean = 6 [default = ""];
  optional RandomGeneratorParameter mirror = 10;
  optional RandomGeneratorParameter translate = 11 ;
  optional RandomGeneratorParameter rotate = 12 ;
  optional RandomGeneratorParameter zoom = 13 ;
  optional RandomGeneratorParameter squeeze = 14 ;
  optional RandomGeneratorParameter lmult_pow = 20 ;
  optional RandomGeneratorParameter lmult_mult = 21 ;
  optional RandomGeneratorParameter lmult_add = 22 ;
  optional RandomGeneratorParameter sat_pow = 23 ;
  optional RandomGeneratorParameter sat_mult = 24 ;
  optional RandomGeneratorParameter sat_add = 25 ;
  optional RandomGeneratorParameter col_pow = 26 ;
  optional RandomGeneratorParameter col_mult = 27 ;
  optional RandomGeneratorParameter col_add = 28 ;
  optional RandomGeneratorParameter ladd_pow = 29 ;
  optional RandomGeneratorParameter ladd_mult = 30 ;
  optional RandomGeneratorParameter ladd_add = 31 ; 
  optional RandomGeneratorParameter col_rotate = 32 ; 
}

// Message storing the actual coefficients of a transformation
message AugmentationCoeff {
  
  optional float mirror = 1 [default = 0];
  optional float dx = 2 [default = 0];
  optional float dy = 3 [default = 0];
  optional float angle = 4 [default = 0];
  optional float zoom_x = 5 [default = 1];
  optional float zoom_y = 6 [default = 1];
  
  optional float pow_nomean0 = 10 [default = 1];
  optional float pow_nomean1 = 11 [default = 1];
  optional float pow_nomean2 = 12 [default = 1];
  optional float add_nomean0 = 13 [default = 0];
  optional float add_nomean1 = 14 [default = 0];
  optional float add_nomean2 = 15 [default = 0];
  optional float mult_nomean0 = 16 [default = 1];
  optional float mult_nomean1 = 17 [default = 1];
  optional float mult_nomean2 = 18 [default = 1];
  
  optional float pow_withmean0 = 19 [default = 1];
  optional float pow_withmean1 = 20 [default = 1];
  optional float pow_withmean2 = 21 [default = 1];
  optional float add_withmean0 = 22 [default = 0];
  optional float add_withmean1 = 23 [default = 0];
  optional float add_withmean2 = 24 [default = 0];
  optional float mult_withmean0 = 25 [default = 1];
  optional float mult_withmean1 = 26 [default = 1];
  optional float mult_withmean2 = 27 [default = 1];
  
  optional float lmult_pow = 28 [default = 1];
  optional float lmult_add = 29 [default = 0];
  optional float lmult_mult = 30 [default = 1];
  optional float col_angle = 31 [default = 0]; 
  
}  

// Message that stores parameters used by ConcatLayer
message ConcatParameter {
  // Concat Layer needs to specify the dimension along the concat will happen,
  // the other dimensions must be the same for all the bottom blobs
  // By default it will concatenate blobs along channels dimension
  optional uint32 concat_dim = 1 [default = 1];
}

message ConditionParameter {
  optional float ge_than = 1 [default = 0.];
  optional float le_than = 2 [default = -1.];
}

// Message that stores parameters used by ConvolutionLayer
message ConvolutionParameter {
  optional uint32 num_output = 1; // The number of outputs for the layer
  optional bool bias_term = 2 [default = true]; // whether to have bias terms
  optional uint32 pad = 3 [default = 0]; // The padding size
  optional uint32 kernel_size = 4; // The kernel size
  optional uint32 group = 5 [default = 1]; // The group size for group conv
  optional uint32 stride = 6 [default = 1]; // The stride
  optional FillerParameter weight_filler = 7; // The filler for the weight
  optional FillerParameter bias_filler = 8; // The filler for the bias
}

message CouplesParameter {
  optional float match_ratio = 1 [default = 0.5];
  optional float scale = 2 [default = 1.];
}

// Message that stores parameters used by DeConvolutionLayer
message DeConvolutionParameter {
  optional uint32 num_output = 1; // The number of outputs for the layer
  optional uint32 output_channels = 11; // The number of outputs for the layer
  optional bool bias_term = 2 [default = true]; // whether to have bias terms
  optional uint32 pad = 3 [default = 0]; // The padding size
  optional uint32 kernel_size = 4; // The kernel size
  optional uint32 group = 5 [default = 1]; // The group size for group conv
  optional uint32 stride = 6 [default = 1]; // The stride
  optional FillerParameter weight_filler = 7; // The filler for the weight
  optional FillerParameter bias_filler = 8; // The filler for the bias
  optional uint32 output_height = 9; // The output height
  optional uint32 output_width = 10; // The output width
}


// Message that stores parameters used by DataLayer
message DataParameter {
  enum DB {
    LEVELDB = 0;
    LMDB = 1;
  }
  // Specify the data source.
  optional string source = 1;
  // For data pre-processing, we can do simple scaling and subtracting the
  // data mean, if provided. Note that the mean subtraction is always carried
  // out before scaling.
  optional float scale = 2 [default = 1];
  optional string mean_file = 3;
  // Specify the batch size.
  optional uint32 batch_size = 4;
  // Specify if we would like to randomly crop an image.
  optional uint32 crop_size = 5 [default = 0];
  // Specify if we want to randomly mirror data.
  optional bool mirror = 6 [default = false];
  // The rand_skip variable is for the data layer to skip a few data points
  // to avoid all asynchronous sgd clients to start at the same point. The skip
  // point would be set as rand_skip * rand(0,1). Note that rand_skip should not
  // be larger than the number of keys in the leveldb.
  optional uint32 rand_skip = 7 [default = 0];
  optional DB backend = 8 [default = LEVELDB];
  // Specify if we want to skip a random number of samples (between 1 and randomize_data_sampling) at each step of loading data 
  optional uint32 randomize_data_sampling = 10 [default = 0];
}

// Message that stores parameters used by DropoutLayer
message DropoutParameter {
  optional float dropout_ratio = 1 [default = 0.5]; // dropout ratio
}

// Message that stores parameters used by DummyDataLayer.
// DummyDataLayer fills any number of arbitrarily shaped blobs with random
// (or constant) data generated by "Fillers" (see "message FillerParameter").
message DummyDataParameter {
  // This layer produces N >= 1 top blobs.  DummyDataParameter must specify 1 or N
  // num, N channels, N height, and N width fields, and must specify 0, 1 or N
  // data_fillers.
  //
  // If 0 data_fillers are specified, ConstantFiller with a value of 0 is used.
  // If 1 data_filler is specified, it is applied to all top blobs.  If N are
  // specified, the ith is applied to the ith top blob.
  repeated FillerParameter data_filler = 1;
  repeated uint32 num = 2;
  repeated uint32 channels = 3;
  repeated uint32 height = 4;
  repeated uint32 width = 5;
}

// Message that stores parameters used by EltwiseLayer
message EltwiseParameter {
  enum EltwiseOp {
    PROD = 0;
    SUM = 1;
  }
  optional EltwiseOp operation = 1 [default = SUM]; // element-wise operation
  repeated float coeff = 2; // blob-wise coefficient for SUM operation
}

// Message that stores parameters used by ThresholdLayer
message ThresholdParameter {
  optional float threshold = 1 [default = 0]; // Strictly Positive values
}

// Message that stores parameters used by HDF5DataLayer
message HDF5DataParameter {
  // Specify the data source.
  optional string source = 1;
  // Specify the batch size.
  optional uint32 batch_size = 2;
  // Load all data/files into memory at startup:
  optional bool data_in_memory = 3 [default = true];
}

// Message that stores parameters used by HDF5OutputLayer
message HDF5OutputParameter {
  optional string file_name = 1;
}

message HingeLossParameter {
  enum Norm {    
    L1 = 1;
    L2 = 2;    
  }
  // Specify the Norm to use L1 or L2
  optional Norm norm = 1 [default = L1];
}

// Message that stores parameters used by ImageDataLayer
message ImageDataParameter {
  // Specify the data source.
  optional string source = 1;
  // For data pre-processing, we can do simple scaling and subtracting the
  // data mean, if provided. Note that the mean subtraction is always carried
  // out before scaling.
  optional float scale = 2 [default = 1];
  optional string mean_file = 3;
  // Specify the batch size.
  optional uint32 batch_size = 4;
  // Specify if we would like to randomly crop an image.
  optional uint32 crop_size = 5 [default = 0];
  // Specify if we want to randomly mirror data.
  optional bool mirror = 6 [default = false];
  // The rand_skip variable is for the data layer to skip a few data points
  // to avoid all asynchronous sgd clients to start at the same point. The skip
  // point would be set as rand_skip * rand(0,1). Note that rand_skip should not
  // be larger than the number of keys in the leveldb.
  optional uint32 rand_skip = 7 [default = 0];
  // Whether or not ImageLayer should shuffle the list of files at every epoch.
  optional bool shuffle = 8 [default = false];
  // It will also resize images if new_height or new_width are not zero.
  optional uint32 new_height = 9 [default = 0];
  optional uint32 new_width = 10 [default = 0];
}

// Message that stores parameters InfogainLossLayer
message InfogainLossParameter {
  // Specify the infogain matrix source.
  optional string source = 1;
}

// Message that stores parameters used by InnerProductLayer
message InnerProductParameter {
  optional uint32 num_output = 1; // The number of outputs for the layer
  optional bool bias_term = 2 [default = true]; // whether to have bias terms
  optional FillerParameter weight_filler = 3; // The filler for the weight
  optional FillerParameter bias_filler = 4; // The filler for the bias
}

message CoeffScheduleParameter {
  optional float gamma = 1 [default = 1];
  optional float initial_coeff = 2 [default = 1];
  optional float final_coeff = 3 [default = 1];
}

message LossParameter {
  // Specify the infogain matrix source.
  optional float coeff = 1 [default = 1.];
}

// Message that stores parameters used by LRNLayer
message LRNParameter {
  optional uint32 local_size = 1 [default = 5];
  optional float alpha = 2 [default = 1.];
  optional float beta = 3 [default = 0.75];
  enum NormRegion {
    ACROSS_CHANNELS = 0;
    WITHIN_CHANNEL = 1;
  }
  optional NormRegion norm_region = 4 [default = ACROSS_CHANNELS];
}

// Message that stores parameters used by MemoryDataLayer
message MemoryDataParameter {
  optional uint32 batch_size = 1;
  optional uint32 channels = 2;
  optional uint32 height = 3;
  optional uint32 width = 4;
}

// Message that stores parameters used by Orth layers
message OrthParameter {
  
  enum OrthMethod {    
    NONE = 1;                // do not orthogonalize
    ESMAEILI = 2;            // as in "A modification for Kovarikâ€™s method" by H. Esmaeili
    NORM_L2 = 3;                // normalize the columns wrt L2 norm, do not orthogonalize
    NORM_L1 = 4;                // normalize the columns wrt L1 norm, do not orthogonalize
    NORM_L1_NONNEG = 5;                // make all elements nonnegative, normalize the columns wrt L1 norm, do not orthogonalize
  }
  optional OrthMethod orth_method = 1 [default = NONE]; // method of orthogonalization
  optional uint32 step = 2 [default = 1]; // how often to perform orthogonalization
  optional uint32 max_num_iter = 3 [default = 1]; // max number of iterations during orthogonalization
  optional float eps = 4 [default = 1e-3]; // target MSE
  optional float esmaeili_coeff = 5 [default = 0.55]; // coefficient to be used by Esmaeli's algorithm
  optional float col_norm = 6 [default = 1.]; // Norm of each column in the resulting orthogonal matrix
  optional float min_norm = 7 [default = 0.]; // normalize if norm is smaller than this
  optional float max_norm = 8 [default = 10.]; //                   or larger than this
  optional float target_norm = 9 [default = 1.]; //           and set the norm to this
  optional bool transpose = 10 [default = false]; // transpose the matrix before orthogonalizing?
  optional uint32 orth_before_iter = 11 [default = 0]; // only do orthogonalization before this iteration (0 means inf)
}

// Message that stores parameters used by PoolingLayer
message PoolingParameter {
  enum PoolMethod {
    MAX = 0;
    AVE = 1;
    STOCHASTIC = 2;
  }
  optional PoolMethod pool = 1 [default = MAX]; // The pooling method
  optional uint32 kernel_size = 2; // The kernel size
  optional uint32 stride = 3 [default = 1]; // The stride
  // The padding size -- currently implemented only for average and max pooling.
  // average pooling zero pads. max pooling -inf pads.
  optional uint32 pad = 4 [default = 0];
}

// Message that stores parameters used by PowerLayer
message PowerParameter {
  // PowerLayer computes outputs y = (shift + scale * x) ^ power.
  optional float power = 1 [default = 1.0];
  optional float scale = 2 [default = 1.0];
  optional float shift = 3 [default = 0.0];
}

// Message used by AugmentationParameter for describing how to generate augmentation parameters
message RandomGeneratorParameter {
  optional string rand_type = 1 [default = "uniform" ]; // can be uniform, gaussian, bernoulli
  optional bool exp = 2 [default = false ]; // after generating the random number, exponentiate it or not 
  optional float mean = 4 [default = 0. ]; // mean of the random variable
  optional float spread = 5 [default = 0. ]; // half of interval length for uniform; standard deviation for gaussian 
  optional float prob = 6 [default = .5];
  optional bool apply_schedule = 7 [default = true];
}

// Message that stores parameters used by WindowDataLayer
message WindowDataParameter {
  // Specify the data source.
  optional string source = 1;
  // For data pre-processing, we can do simple scaling and subtracting the
  // data mean, if provided. Note that the mean subtraction is always carried
  // out before scaling.
  optional float scale = 2 [default = 1];
  optional string mean_file = 3;
  // Specify the batch size.
  optional uint32 batch_size = 4;
  // Specify if we would like to randomly crop an image.
  optional uint32 crop_size = 5 [default = 0];
  // Specify if we want to randomly mirror data.
  optional bool mirror = 6 [default = false];
  // Foreground (object) overlap threshold
  optional float fg_threshold = 7 [default = 0.5];
  // Background (non-object) overlap threshold
  optional float bg_threshold = 8 [default = 0.5];
  // Fraction of batch that should be foreground objects
  optional float fg_fraction = 9 [default = 0.25];
  // Amount of contextual padding to add around a window
  // (used only by the window_data_layer)
  optional uint32 context_pad = 10 [default = 0];
  // Mode for cropping out a detection window
  // warp: cropped window is warped to a fixed size and aspect ratio
  // square: the tightest square around the window is cropped
  optional string crop_mode = 11 [default = "warp"];
}

// Message that stores parameters used by ReLULayer
message ReLUParameter {
  // Allow non-zero slope for negative inputs to speed up optimization
  // Described in:
  // Maas, A. L., Hannun, A. Y., & Ng, A. Y. (2013). Rectifier nonlinearities
  // improve neural network acoustic models. In ICML Workshop on Deep Learning
  // for Audio, Speech, and Language Processing.
  optional float negative_slope = 1 [default = 0];
  enum Engine {
    DEFAULT = 0;
    CAFFE = 1;
    CUDNN = 2;
  }
  optional Engine engine = 2 [default = DEFAULT];
}

message ReshapeParameter {
  optional uint32 channels = 1;
  optional uint32 height = 2;
  optional uint32 width = 3;
}

// DEPRECATED: V0LayerParameter is the old way of specifying layer parameters
// in Caffe.  We keep this message type around for legacy support.
message V0LayerParameter {
  optional string name = 1; // the layer name
  optional string type = 2; // the string to specify the layer type

  // Parameters to specify layers with inner products.
  optional uint32 num_output = 3; // The number of outputs for the layer
  optional bool biasterm = 4 [default = true]; // whether to have bias terms
  optional FillerParameter weight_filler = 5; // The filler for the weight
  optional FillerParameter bias_filler = 6; // The filler for the bias

  optional uint32 pad = 7 [default = 0]; // The padding size
  optional uint32 kernelsize = 8; // The kernel size
  optional uint32 group = 9 [default = 1]; // The group size for group conv
  optional uint32 stride = 10 [default = 1]; // The stride
  enum PoolMethod {
    MAX = 0;
    AVE = 1;
    STOCHASTIC = 2;
  }
  optional PoolMethod pool = 11 [default = MAX]; // The pooling method
  optional float dropout_ratio = 12 [default = 0.5]; // dropout ratio

  optional uint32 local_size = 13 [default = 5]; // for local response norm
  optional float alpha = 14 [default = 1.]; // for local response norm
  optional float beta = 15 [default = 0.75]; // for local response norm

  // For data layers, specify the data source
  optional string source = 16;
  // For data pre-processing, we can do simple scaling and subtracting the
  // data mean, if provided. Note that the mean subtraction is always carried
  // out before scaling.
  optional float scale = 17 [default = 1];
  optional string meanfile = 18;
  // For data layers, specify the batch size.
  optional uint32 batchsize = 19;
  // For data layers, specify if we would like to randomly crop an image.
  optional uint32 cropsize = 20 [default = 0];
  // For data layers, specify if we want to randomly mirror data.
  optional bool mirror = 21 [default = false];

  // The blobs containing the numeric parameters of the layer
  repeated BlobProto blobs = 50;
  // The ratio that is multiplied on the global learning rate. If you want to
  // set the learning ratio for one blob, you need to set it for all blobs.
  repeated float blobs_lr = 51;
  // The weight decay that is multiplied on the global weight decay.
  repeated float weight_decay = 52;

  // The rand_skip variable is for the data layer to skip a few data points
  // to avoid all asynchronous sgd clients to start at the same point. The skip
  // point would be set as rand_skip * rand(0,1). Note that rand_skip should not
  // be larger than the number of keys in the leveldb.
  optional uint32 rand_skip = 53 [default = 0];

  // Fields related to detection (det_*)
  // foreground (object) overlap threshold
  optional float det_fg_threshold = 54 [default = 0.5];
  // background (non-object) overlap threshold
  optional float det_bg_threshold = 55 [default = 0.5];
  // Fraction of batch that should be foreground objects
  optional float det_fg_fraction = 56 [default = 0.25];

  // optional bool OBSOLETE_can_clobber = 57 [default = true];

  // Amount of contextual padding to add around a window
  // (used only by the window_data_layer)
  optional uint32 det_context_pad = 58 [default = 0];

  // Mode for cropping out a detection window
  // warp: cropped window is warped to a fixed size and aspect ratio
  // square: the tightest square around the window is cropped
  optional string det_crop_mode = 59 [default = "warp"];

  // For ReshapeLayer, one needs to specify the new dimensions.
  optional int32 new_num = 60 [default = 0];
  optional int32 new_channels = 61 [default = 0];
  optional int32 new_height = 62 [default = 0];
  optional int32 new_width = 63 [default = 0];

  // Whether or not ImageLayer should shuffle the list of files at every epoch.
  // It will also resize images if new_height or new_width are not zero.
  optional bool shuffle_images = 64 [default = false];

  // For ConcatLayer, one needs to specify the dimension for concatenation, and
  // the other dimensions must be the same for all the bottom blobs.
  // By default it will concatenate blobs along the channels dimension.
  optional uint32 concat_dim = 65 [default = 1];

  optional HDF5OutputParameter hdf5_output_param = 1001;
}
